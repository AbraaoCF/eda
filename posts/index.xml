
   <rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
     <channel>
       <title>Posts on Estrutura de Dados</title>
       <link>https://joaoarthurbm.github.io/eda/posts/</link>
       <description>Recent content in Posts on Estrutura de Dados</description>
       <generator>Hugo -- gohugo.io</generator>
       <copyright>Copyright &amp;copy; 2019 - João Arthur Brunet</copyright>
       <lastBuildDate>Wed, 30 Oct 2019 00:00:00 -0300</lastBuildDate>
       
           <atom:link href="https://joaoarthurbm.github.io/eda/posts/index.xml" rel="self" type="application/rss+xml" />
       
       
       <item>
         <title>Introdução à Análise de Algoritmos</title>
         <link>https://joaoarthurbm.github.io/eda/posts/introducao-a-analise/</link>
         <pubDate>Wed, 30 Oct 2019 00:00:00 -0300</pubDate>
         
         <guid>https://joaoarthurbm.github.io/eda/posts/introducao-a-analise/</guid>
         <description>

&lt;hr /&gt;

&lt;p&gt;A análise de eficiência é uma etapa fundamental na concepção de um algoritmo. Embora aspectos como legibilidade, simplicidade e modularidade de uma solução sejam importantes para a sua manutenabilidade, a eficiência de uma solução desempenha um papel muito relevante para a sua adoção.&lt;/p&gt;

&lt;p&gt;Diante de um problema computacional, diversas soluções podem ser propostas. Por exemplo, para ordenar um sequência de números, o desenvolvedor pode utilizar algoritmos como o BubbleSort, MergeSort, QuickSort entre outros. Entender como esses algoritmos se comportam à medida que aumentamos o tamanho da entrada a ser ordenada é primordial para decidirmos qual solução adotar em um determinado contexto.&lt;/p&gt;

&lt;p&gt;Analisar um algoritmo significa prever a quantidade de recursos que tal algoritmo consome ao ser executado. A análise pode apontar diversos candidatos e, tipicamente, exclui diversas soluções não eficientes. Diversas variáveis podem ser objetos de estudo da análise de um algoritmo, por exemplo, consumo de memória, largura de banda de comunicação entre outros. No entanto, com frequência, desejamos medir o tempo execução. E é essa variável que estamos interessados em discutir neste documento.&lt;/p&gt;

&lt;p&gt;Uma abordagem direta para analisar o desempenho de um algoritmo é a abordagem empírica. Neste caso, configura-se um ambiente em que as variáveis são controladas e executa-se os algoritmos com o intuito de medir o tempo de computação e comparar as diferentes soluções.
O tempo de execução (eixo y) é medido em função do tamanho da entrada (eixo x). Por exemplo, para analisar empiricamente um algoritmo de ordenação medimos o tempo de execução para diferentes tamanhos de arrays. Além disso, podemos querer variar a configuração do array sob ordenação para entender, por exemplo, como o algoritmo se comporta com um array já ordenado ou como se comporta com arrays parcialmente ordenados.&lt;/p&gt;

&lt;p&gt;Tipicamente, executa-se um experimento com o tamanho da amostra suficiente para se ter validade estatística e permitir a construção de um modelo que represente a curva de cada algoritmo. A Figura abaixo apresenta os tempos de computação de diferentes algoritmos de ordenação à medida que aumenta-se o tamanho da entrada. Como podemos notar, o algoritmo SelectionSort apresenta tempo de execução consideravelmente maior em comparação com as outras três alternativas à medida que a quantidade de elementos a serem ordenados cresce.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;comparacao-ordenacao.jpeg&#34; alt=&#34;comp-ordenacao&#34; title=&#34;Comparação de Algoritmos de Ordenação&#34; /&gt;&lt;/p&gt;

&lt;p&gt;A abordagem empírica para análise de algoritmos é útil, pois, se conduzida de maneira metodologicamente apropriada, fornece valores precisos sobre o tempo de execução de um algoritmo. No entanto, essa abordagem apresenta algumas desvantagens. Primeiro, existe um alto custo relacionado à implementação de todos os algoritmos, além da configuração, execução e análise do experimento. Além disso, note que as conclusões são limitadas ao espaço de entrada do experimento. Por fim, os resultados são dependentes do hardware utilizado.&lt;/p&gt;

&lt;p&gt;Diante do cenário exposto acima, surge a necessidade de uma análise que:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;seja independente de hardware;&lt;/li&gt;
&lt;li&gt;permita analisar os algoritmos em um espectro maior de entradas;&lt;/li&gt;
&lt;li&gt;seja simples.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Note que, em diversas situações, o interesse está em comparar algoritmos, ao invés de determinar o seu tempo exato de execução. Em particular, &lt;strong&gt;estamos interessados nas funções no comportamento dos algoritmos para grandes tamanhos de entrada &amp;ndash; análise assintótica.&lt;/strong&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&#34;análise-de-algoritmos&#34;&gt;Análise de Algoritmos&lt;/h2&gt;

&lt;p&gt;Antes de apresentar os conceitos de análise assintótica, sua notação e modus operandi, é preciso apresentar a hipótese em que a análise de algoritmos se baseia:&lt;/p&gt;

&lt;p align=&#34;center&#34;&gt;Hipótese: O custo de operações primitivas é constante.&lt;/p&gt;

&lt;p&gt;Essa hipótese estabelece que operações aritméticas, indexação de elementos em um vetor, retorno de métodos, atribuição de valores às variáveis, comparação de elementos, entre outros, executam em tempo constante, referenciado como $O(1)$ ou $O(C)$. É importante destacar que, na prática, esse custo varia de acordo com o hardware, linguagem de programação etc. No entanto, essa variação é insignificante do ponto de vista da análise assintótica. O quadro abaixo lista as operações primitivas detalhadamente.&lt;/p&gt;

&lt;pre&gt;
Operações Primitivas

* Avaliação de expressões booleanas (i &gt;= 2; i == 2, etc);

* Operações matemáticas (*, -, +, %, etc);

* Retorno de métodos (return x;);

* Atribuição (i = 2);

* Acesso à variáveis e posições arbitrárias de um array (v[i]).

&lt;/pre&gt;

&lt;p&gt;Nesse contexto, o tempo de execução de um algoritmo é a soma do custo das operações primitivas. Por exemplo, considere o algoritmo que multiplica o resto da divisão de dois inteiros pela parte inteira da mesma divisão:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-java&#34; data-lang=&#34;java&#34;&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;multiplicaRestoPorParteInteira&lt;/span&gt;(&lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;i&lt;/span&gt;, &lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;j&lt;/span&gt;) {
    &lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;resto&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; i &lt;span style=&#34;color:#f92672&#34;&gt;%&lt;/span&gt; j;
    &lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;pInteira&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; i &lt;span style=&#34;color:#f92672&#34;&gt;/&lt;/span&gt; j;
    &lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;resultado&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; resto &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt; pInteira;
    &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; resultado;
}&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;strong&gt;Passo 1: Identificar primitivas.&lt;/strong&gt; O primeiro passo para determinar de modo analítico o tempo de execução de qualquer algoritmo é identificar todas as operações primitivas. Cada uma, como discutido anteriormente, tem um custo constante. Para o algoritmo acima temos:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;atribuição (resto = ) -&amp;gt; $c1$&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;operação aritmética (i % j) -&amp;gt; $c2$&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;atribuição (pInteira = ) -&amp;gt; $c3$&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;operação aritmética (i % j) -&amp;gt; $c4$&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;atribuição (resultado = ) -&amp;gt; $c5$&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;operação aritmética (resto * pInteira) -&amp;gt; $c6$&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;retorno de método (return resultado) -&amp;gt; $c7$&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;strong&gt;Passo 2: Identificar a quantidade de vezes que cada uma das primitivas é executada.&lt;/strong&gt; Para o algoritmo acima, todas as primitivas são executadas apenas uma vez.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Passo 3: Somar o custo total.&lt;/strong&gt; O tempo de execução do algoritmo é a soma das execuções das operações primitivas. Nesse caso temos que a função que descreve o tempo de execução é:&lt;/p&gt;

&lt;p align=&#34;center&#34;&gt;$f(n) = c1+c2+c3+c4+c5+c6+c7$&lt;/p&gt;

&lt;p&gt;Lembrando estamos interessados em uma função que nos diga o tempo de execução em relação ao tamanho da entrada. Nesse caso, escolhemos $n$ para representar o tamanho da entrada. Como pode ser visto na função detalhada, o custo não depende de $n$ de maneira alguma. Independente dos números passados como parâmetro, o custo será sempre o mesmo. Por isso dizemos que essa função, e portanto o algoritmo que é descrito por ela, tem &lt;strong&gt;custo constante&lt;/strong&gt;, ou seja, independe do tamanho da entrada.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Dizer que um algoritmo tem custo constante significa dizer que o seu tempo de execução independe do tamanho da entrada.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Outro fator de destaque é que podemos considerar que todas as constantes possuem o mesmo valor $c$. Assim, podemos simplificar a função para $f(n)= 7c$.&lt;/p&gt;

&lt;h3 id=&#34;e-quando-houver-condicionais&#34;&gt;E quando houver condicionais?&lt;/h3&gt;

&lt;p&gt;O uso de comandos condicionais é muito comum em nossos algoritmos e nos impõe uma dificuldade na análise do tempo de execução. Essa dificuldade está relacionada ao fato de que, dependendo do caso, apenas uma parte do código é executada. Como decidir como fazer a análise? Que caminho devemos computar?&lt;/p&gt;

&lt;p&gt;Nesse caso, escolhemos &lt;strong&gt;o pior caso&lt;/strong&gt;. Neste curso estamos interessados em saber como os algoritmos se comportam no seu pior caso. A análise do pior caso é útil para eliminarmos soluções ruins. Além disso, o melhor caso raramente acontece, ao contrário dos outros casos que podem ser bem mais comuns. Por último, o caso médio, além de demandar análise estatística, muitas vezes é muito semelhante ao pior caso.&lt;/p&gt;

&lt;p&gt;Para demonstrar a análise de pior caso, vamos analisar um método que recebe as três notas de um aluno e calcula a nota que ele precisa obter na prova final, se esse for o caso. Se o aluno for aprovado ($media &amp;gt;= 7.0$) ou reprovado sem direito a final ($media &amp;lt; 4$), o método deve retornar $0$.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-java&#34; data-lang=&#34;java&#34;&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;double&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;precisaNaFinal&lt;/span&gt;(&lt;span style=&#34;color:#66d9ef&#34;&gt;double&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;nota1&lt;/span&gt;, &lt;span style=&#34;color:#66d9ef&#34;&gt;double&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;nota2&lt;/span&gt;, &lt;span style=&#34;color:#66d9ef&#34;&gt;double&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;nota3&lt;/span&gt;) {

    &lt;span style=&#34;color:#66d9ef&#34;&gt;double&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;media&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; (nota1 &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; nota2 &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; nota3) &lt;span style=&#34;color:#f92672&#34;&gt;/&lt;/span&gt; 3;
        
    &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; (media &lt;span style=&#34;color:#f92672&#34;&gt;&amp;gt;=&lt;/span&gt; 7 &lt;span style=&#34;color:#f92672&#34;&gt;||&lt;/span&gt; media &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&lt;/span&gt; 4) {
        &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; 0;
        
    } &lt;span style=&#34;color:#66d9ef&#34;&gt;else&lt;/span&gt; {
        &lt;span style=&#34;color:#66d9ef&#34;&gt;double&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;mediaFinal&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; 5;
        &lt;span style=&#34;color:#66d9ef&#34;&gt;double&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;pesoFinal&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; 0.&lt;span style=&#34;color:#a6e22e&#34;&gt;4&lt;/span&gt;;
        &lt;span style=&#34;color:#66d9ef&#34;&gt;double&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;pesoMedia&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; 0.&lt;span style=&#34;color:#a6e22e&#34;&gt;6&lt;/span&gt;;
        &lt;span style=&#34;color:#66d9ef&#34;&gt;double&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;precisa&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; (mediaFinal &lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt; pesoMedia &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt; media) &lt;span style=&#34;color:#f92672&#34;&gt;/&lt;/span&gt; pesoFinal;
            
        &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; precisa;
    }

}&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;strong&gt;Passo 1. Identificar primitivas.&lt;/strong&gt;&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;atribuição (media = ) -&amp;gt; $c1$&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;operação aritmética (nota1 + nota2 + nota3) -&amp;gt; $c2$&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;operação aritmética (&amp;hellip; / 3) -&amp;gt; $c3$&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;avaliação de expressão booleana (media &amp;gt;=7 || media &amp;lt; 4) -&amp;gt; $c4$&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;retorno de método (return 0) -&amp;gt; $c5$&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;atribuição (mediaFinal = ) -&amp;gt; $c6$&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;atribuição (pesoFinal = ) -&amp;gt; $c7$&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;atribuição (pesoMedia = ) -&amp;gt; $c8$&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;atribuição (precisa = ) -&amp;gt; $c9$&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;operação aritmética (pesoMedia * media) -&amp;gt; $c10$&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;operação aritmética (mediaFinal -  &amp;hellip;) -&amp;gt; $c11$&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;operação aritmética (&amp;hellip; / pesoFinal) -&amp;gt; $c12$&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;retorno de método (return precisa) -&amp;gt; $c13$&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;strong&gt;Passo 2: Identificar a quantidade de vezes que cada uma das primitivas é executada.&lt;/strong&gt; Aqui vem a grande diferença. Como estamos interessados no pior caso, nós vamos descartar a constante $c5$, pois, no pior caso, o bloco do &lt;code&gt;else&lt;/code&gt; será executado, uma vez que é mais custoso que o bloco do &lt;code&gt;if&lt;/code&gt;. As outras primitivas são executadas apenas uma vez.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Passo 3: Somar o custo total.&lt;/strong&gt;&lt;/p&gt;

&lt;p align=&#34;center&#34;&gt; $f(n) = c1+c2+c3+c4+c6+c7+c8+c9+c10+c11+c12+c13$ &lt;/p&gt;

&lt;h3 id=&#34;e-quando-houver-iteração&#34;&gt;E quando houver iteração?&lt;/h3&gt;

&lt;p&gt;Nos dois exemplos que vimos até aqui todas as primitivas são executadas apenas uma vez e, por isso, o tempo de execução do algoritmo é sempre constante. Vejamos o que acontece quando há iteração. O código abaixo procura por um elemento em um array.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-java&#34; data-lang=&#34;java&#34;&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;public&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;static&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;boolean&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;contains&lt;/span&gt;(&lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt;[] &lt;span style=&#34;color:#a6e22e&#34;&gt;v&lt;/span&gt;, &lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;n&lt;/span&gt;) {
    &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; (&lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;i&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; 0; i &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&lt;/span&gt; v.&lt;span style=&#34;color:#a6e22e&#34;&gt;length&lt;/span&gt;; i&lt;span style=&#34;color:#f92672&#34;&gt;++&lt;/span&gt;)
        &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; (v[i] &lt;span style=&#34;color:#f92672&#34;&gt;==&lt;/span&gt; n)
            &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;true&lt;/span&gt;;
    &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;false&lt;/span&gt;;
}&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;strong&gt;Passo 1: Identificar primitivas.&lt;/strong&gt;&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;Atribuição (int i = 0) -&amp;gt; $c1$&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Avaliação de expressão booleana (i &amp;lt; v.length) -&amp;gt; $c2$&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Operação aritmética (i++) -&amp;gt; $c3$&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Avaliação de expressão booleana (v[i] == n) -&amp;gt; $c4$&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Retorno de método (return true) -&amp;gt; $c5$&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Retorno de método (return false) -&amp;gt; $c6$&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;strong&gt;Passo 2: Identificar a quantidade de vezes que cada uma das primitivas é executada.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Aqui mora a grande diferença da análise deste exemplo em relação aos demais. Em primeiro lugar, nem todas as primitivas são executadas apenas uma vez. Depois, temos que voltar a lembrar que estamos tratando do pior caso. Esse cenário é representado por um array que não contém o número procurado, pois o algoritmo irá realizar todas as iterações e retornar &lt;code&gt;false&lt;/code&gt; no final. Veja que se o número procurado estiver presente, a execução pode terminar bem antes do fim da iteração no array. Isso significa que na nossa análise vamos descartar a primitiva $c5$, pois no pior caso ela nunca é executada.&lt;/p&gt;

&lt;p&gt;Dado que o tamanho do vetor (&lt;code&gt;v.length&lt;/code&gt;) é $n$, temos:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;$c1$ é executada apenas uma vez.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;$c2$ é executada $(n+1)$ vezes. Exemplo: se $n = 5$, temos as seguintes verificações: 0 &amp;lt; 5, 1 &amp;lt; 5; 2 &amp;lt; 5, 3 &amp;lt; 5, 4 &amp;lt; 5 e 5 &amp;lt; 5, quando encerra-se o loop. Ou seja, 6 verificações.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;$c3$ é executada $n$ vezes. Exemplo: se $n = 5$, temos os seguintes incrementos em i: 1, 2, 3, 4 e 5, quando encerra-se o loop.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;$c4$ é executada $n$ vezes.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;No pior caso, $c5$ não é executada.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;$c6$ é executada apenas uma vez.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Passo 3: Somar o custo total.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;O tempo de execução do algoritmo é a soma das execuções das operações primitivas. Nesse caso temos que a função que descreve o tempo de execução é:&lt;/p&gt;

&lt;p align=&#34;center&#34;&gt; $f(n) = c1+c2*(n+1)+c3*n+c4*n+c6$ &lt;/p&gt;

&lt;p&gt;Considerando todas as primitivas com custo $c$ e simplificando a função, temos:&lt;/p&gt;

&lt;p align=&#34;center&#34;&gt; $f(n) = 3*c*n+3*c$ &lt;/p&gt;

&lt;p&gt;Veja que essa função é diretamente relacionada ao tamanho do array ($n$). À medida que cresce o tamanho de $n$, cresce também o tempo de execução do pior caso. Esse crescimento é linear, pois a função é linear. Faz sentido, certo? Iterar em um array com 100 posições é 10 vezes mais lento que iterar em um array de 10 posições. Não é por acaso que o nome desse algoritmo é busca linear. O termo refere-se a ambos: i) a estratégia de procurar o elemento de modo sequencial em uma coleção e ii) o tempo de execução do algoritmo.&lt;/p&gt;

&lt;p&gt;Vamos ver mais um exemplo.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-java&#34; data-lang=&#34;java&#34;&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;public&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;boolean&lt;/span&gt; contemDuplicacao(&lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt;[] &lt;span style=&#34;color:#a6e22e&#34;&gt;v&lt;/span&gt;) {
    &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; (&lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;i&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; 0; i &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&lt;/span&gt; v.&lt;span style=&#34;color:#a6e22e&#34;&gt;length&lt;/span&gt;; i&lt;span style=&#34;color:#f92672&#34;&gt;++&lt;/span&gt;)
        &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; (&lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;j&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; i &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; 1; j &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&lt;/span&gt; v.&lt;span style=&#34;color:#a6e22e&#34;&gt;length&lt;/span&gt;; j&lt;span style=&#34;color:#f92672&#34;&gt;++&lt;/span&gt;)
            &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; (v[i] &lt;span style=&#34;color:#f92672&#34;&gt;==&lt;/span&gt; v[j])
                &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;true&lt;/span&gt;;
    &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;false&lt;/span&gt;;
}&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;strong&gt;Passo 1: Identificar primitivas.&lt;/strong&gt;&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;Atribuição (int i = 0) -&amp;gt; c1&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Avaliação de expressão booleana (i &amp;lt; v.length) -&amp;gt; c2&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Operação aritmética (i++) -&amp;gt; c3&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Atribuição (int j = i + 1) -&amp;gt; c4&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Avaliação de expressão booleana (j &amp;lt; v.length) -&amp;gt; c5&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Operação aritmética (j++) -&amp;gt; c6&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Avaliação de expressão booleana (v[i] == v[j]) -&amp;gt; c7&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Retorno de método (return true) -&amp;gt; c8&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Retorno de método (return false) -&amp;gt; c9&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;strong&gt;Passo 2: Identificar a quantidade de vezes que cada uma das primitivas é executada.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;O pior caso de execução desse algoritmo é quando não há repetição de valores no array. Ou seja, os loops são executados até o final. Então, como estamos falando do pior caso, descartamos $c8$, porque no pior caso essa primitiva nunca será executada.&lt;/p&gt;

&lt;p&gt;Dado que o tamanho do vetor (v.length) é $n$, temos:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;$c1$ é executada apenas uma vez.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;$c2$ é executada $(n+1)$ vezes. Exemplo: se $n = 5$, temos as seguintes verificações: 0 &amp;lt; 5, 1 &amp;lt; 5; 2 &amp;lt; 5, 3 &amp;lt; 5, 4 &amp;lt; 5 e 5 &amp;lt; 5, quando encerra-se o loop. Ou seja, 6 verificações.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;$c3$ é executada $n$ vezes. Exemplo: se $n = 5$, temos os seguintes incrementos em i: 1, 2, 3, 4 e 5, quando encerra-se o loop.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Agora, atenção, porque vamos tratar das primitivas do laço mais interno.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;A quantidade de vezes que $c4$ é executada depende do laço mais externo, pois $j$ varia de acordo com $i$ ($j = i+1$). Como o laço externo executa $n$ vezes, a quantidade de vezes que $j$ varia é dada por: $(n - 1) + (n - 2) + (n - 3) + (n-4) + &amp;hellip;1$. Essa série representa uma Progressão Aritmética finita decrescente com razão 1. A soma de uma PA com essas características é dada por $S = n/2 * (a1+an)$, onde $a1$ e $an$ são o primeiro e o último elemento da sequência, respectivamente. Assim, para $a1=1$ e $an = n-1$, temos que $c4$ é executada ${n^2}/{2}$ vezes.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Como $c5$ é executada uma vez a mais que $c4$ por causa do último teste para sair do laço, então temos que o primeiro termo da PA é $a1 = 1$ e $an = n$. Assim, temos que $c5$ é executada $({n^2 + n})/{2}$.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;$c6$ e $c7$ são executadas a mesma quantidade de vezes que $c4$.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;$c8$ não é executada nenhuma vez porque estamos falando do pior caso&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;$c9$ é executada apenas uma vez.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Passo 3: Somar o custo total.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;O tempo de execução do algoritmo é a soma das execuções das operações primitivas. Nesse caso temos que a função que descreve o tempo de execução é:&lt;/p&gt;

&lt;p&gt;$f(n) = c1 + c2*(n+1) + c3*n + c4 * {n^2}/2 + c5 * (n^2 + n)/2 + c6 * {n^2}/2 + c7 * {n^2}/2 + c9$&lt;/p&gt;

&lt;p&gt;Considerando todas as primitivas com custo c e simplificando a função, temos:&lt;/p&gt;

&lt;p align=&#34;center&#34;&gt; $f(n) = 3 * c + 2 * c * n + 3 * {n^2}/2 + c * (n^2 + n)/2$ &lt;/p&gt;

&lt;p&gt;Veja que essa função é diretamente relacionada ao tamanho do array (n). À medida que cresce o tamanho de $n$, cresce também o tempo de execução do pior caso. O tempo de execução do algoritmo cresce de forma quadrática em relação ao tamanho da entrada, pois a função é quadrática. Faz sentido, certo? Comparar cada elemento de um array com todos os outros é da ordem de $n^2$.&lt;/p&gt;

&lt;p&gt;É importante que você entenda que esse algoritmo é bem mais lento do que o anterior, pois uma função quadrática cresce mais rapidamente que uma função linear.&lt;/p&gt;

&lt;p&gt;No material sobre &lt;a href=&#34;http://joaoarthurbm.github.io/eda/posts/analise-assintotica&#34;&gt;Análise Assintótica&lt;/a&gt; vamos aprender que essa função complicada pode ser simplificada para $n^2$ quando tratamos de grandes entradas, pois as constantes e os expoentes de menor magnitude não impactam muito nesse cenário.&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&#34;resumo&#34;&gt;Resumo&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Calcular o tempo de execução de um algoritmo é muito importante.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Uma forma de calcular o tempo de execução é seguir os passos:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Identificar primitivas&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Identificar o número de vezes que cada uma das primitivas é executada&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Somar o custo total&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;É preciso estar atento para o fato de que estamos falando de análise do pior caso e, por isso, descartamos os fluxos alternativos de menor custo.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
       </item>
       
       <item>
         <title>Análise Assintótica</title>
         <link>https://joaoarthurbm.github.io/eda/posts/analise-assintotica/</link>
         <pubDate>Tue, 29 Oct 2019 00:00:00 -0300</pubDate>
         
         <guid>https://joaoarthurbm.github.io/eda/posts/analise-assintotica/</guid>
         <description>

&lt;hr /&gt;

&lt;p&gt;&lt;strong&gt;O Problema.&lt;/strong&gt; No &lt;a href=&#34;http://joaoarthurbm.github.io/eda/posts/introducao-a-analise/&#34;&gt;material introdutório de análise de algoritmos&lt;/a&gt; aprendemos a definir a função que descreve o custo de execução de algoritmos. Vimos exemplos simples cujas funções são também simples. Contudo, vamos supor que a função que descreve o tempo de execução de um algoritmo é dada por:&lt;/p&gt;

&lt;p align=&#34;center&#34;&gt; $1.1 * n^2 + (10 + \sin(n + 15) * n^{1.5}) + 9000$ &lt;/p&gt;

&lt;p&gt;Você há de convir que não é simples olhar para essa função e ter uma ideia clara do crescimento dela, certo? Lembra que nossa motivação para estabelecer essa função é simplificar nossa vida. É olhar para a função e ter uma ideia de como ela se comporta à medida que o tamanho da entrada cresce. Então, temos um problema aqui. Essa função não está ajudando nossa vida. Precisamos simplificá-la.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;A Solução.&lt;/strong&gt; Simplificar. Na verdade, nós já utilizamos algumas simplificações para facilitar a análise de algoritmos. A principal delas é ignorar o custo real das operações primitivas, utilizando a constante C ou 1. Agora, faremos mais uma abstração simplificadora, chamada &lt;strong&gt;ordem de crescimento&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;A ideia é determinar como o algoritmo se comporta para valores muito grandes de entrada. Neste caso, ignoramos as constantes e os valores de menor magnitude por entender que eles não são significativos diante dos valores de maior magnitude.&lt;/p&gt;

&lt;p&gt;Na prática, isso significa dizer que podemos:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;ignorar as constantes;&lt;/li&gt;
&lt;li&gt;ignorar os expoentes de menor magnitude.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;No exemplo fictício da função acima, podemos então fazer as seguintes simplificações.&lt;/p&gt;

&lt;p&gt;f(n) = &lt;del&gt;1.1&lt;/del&gt; * n ** 2 &lt;del&gt;+ (10 + sin(n+15)*n**1.5) + 9000&lt;/del&gt;&lt;/p&gt;

&lt;p&gt;Ou seja, do ponto de vista da ordem de crescimento, para grandes valores de $n$, as constantes e os expoentes de maior magnitude são insignificantes, nos permitindo simplificar a expressão do tempo de execução para $\Theta(n^2)$. A notação theta será discutida com mais detalhes na próxima seção.&lt;/p&gt;

&lt;p&gt;Agora sim, é muito mais direto olhar para $n^2$ ter uma ideia clara do crescimento do tempo de execução do algoritmo, porque $n^2$ é uma função que já conhecemos bastante.&lt;/p&gt;

&lt;p&gt;O interesse está na ordem de crescimento das funções, que facilita a análise e comparação de diferentes soluções. Por exemplo, na escolha entre uma solução $\Theta(\log n)$ e uma solução $\Theta(n)$, é preferível a primeira, pois à medida que o tamanho da entrada cresce, o tempo de execução cresce mais lentamente que $\Theta(n)$.&lt;/p&gt;

&lt;p&gt;&amp;ldquo;Ainda não entendi. Você está querendo dizer que $f(n) = 1.1 * n^2 + (10 + \sin(n + 15) * n^{1.5}) + 9000$ e $g(n) = n^2$ são iguais?&amp;rdquo;&lt;/p&gt;

&lt;p&gt;Não. Eu estou dizendo que elas pertencem à mesma classe de funções, as funções quadráticas. Eu estou querendo dizer que essas duas funções possuem a mesma ordem de crescimento para grandes entradas e que se aproximam muito uma da outra para grandes valores de $n$.&lt;/p&gt;

&lt;p&gt;Vou te mostrar. Os gráficos da sequência abaixo ilustra essas duas funções. $f(n)$ está destacada em azul e $g(n)$ em vermelho. A única diferença é que a entrada (eixo x) vai aumentando de um gráfico para outro.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;passo1.png&#34; alt=&#34;passo1&#34; /&gt;
&lt;img src=&#34;passo2.png&#34; alt=&#34;passo2&#34; /&gt;
&lt;img src=&#34;passo3.png&#34; alt=&#34;passo3&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Note que no primeiro gráfico, para entradas de 0 a 200, $f(x)$ tem uma curva semelhante à $g(x)$, mas estão distantes uma da outra. Depois, na segunda figura, a entrada passa a variar de 0 a 300 e já conseguimos notar uma aproximação dessas duas curvas. Por fim,  no último gráfico, com entradas variando de 0 a 400, as duas curvas já estão muito próximas uma da outra.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Outro exemplo.&lt;/strong&gt; Através da identificação das operações primitivas e da quantidade de vezes que são executadas no algoritmo de busca linear chegamos à seguinte função:&lt;/p&gt;

&lt;p align=&#34;center&#34;&gt;$f(n) = 3*c*n + 3*c$&lt;/p&gt;

&lt;p&gt;Aplicando as diretrizes simplificação, temos:&lt;/p&gt;

&lt;p&gt;f(n) = &lt;del&gt;3 * c *&lt;/del&gt; n &lt;del&gt;+ 3 * c&lt;/del&gt;&lt;/p&gt;

&lt;p&gt;Isto é, $f(n) = 3*c*n+3*c$ é $\Theta(n)$. Isso significa dizer que f(n) tem a mesma ordem de crescimento que uma função linear.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Dizer que um algoritmo é linear significa que o tempo de execução do algoritmo cresce linearmente em função do tamanho da entrada.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;strong&gt;Mais um exemplo.&lt;/strong&gt; Através da identificação das operações primitivas e da quantidade de vezes que são executadas no algoritmo de identificação de elementos duplicados em um array chegamos à seguinte função:&lt;/p&gt;

&lt;p align=&#34;center&#34;&gt; $f(n) = 3 * c + 2 * c * n + 3 * {n^2}/2 + c * (n^2 + n)/2$ &lt;/p&gt;

&lt;p&gt;Aplicando as diretrizes simplificação, temos que $f(n) = \Theta(n^2)$. Isso significa dizer que $f(n)$ tem a mesma ordem de crescimento que uma quadrática.&lt;/p&gt;

&lt;hr /&gt;

&lt;h1 id=&#34;análise-assintótica&#34;&gt;Análise Assintótica&lt;/h1&gt;

&lt;p&gt;Quando observamos tamanhos de entrada grande o suficiente para tornar relevante apenas a ordem de crescimento do tempo de execução, estamos estudando a eficiência assintótica.&lt;/p&gt;

&lt;p&gt;Há pouco, aplicando as diretrizes de simplificação, aprendemos que funções complexas podem ser mapeadas para classes de funções sobre as quais conhecemos o crescimento ($n$, $\log n$, $n^2$ etc). Para ilustrar esse mapeamento utilizamos a notação $\Theta$. Chegou a hora de entendermos o que essa notação significa.&lt;/p&gt;

&lt;p&gt;Primeiro, preciso deixar claro que cometi alguns abusos matemáticos para fins didáticos. Vamos relembrar esses abusos e explicá-los um a um.&lt;/p&gt;

&lt;p align=&#34;center&#34;&gt;$2*n + 1 = \Theta(n)$&lt;/p&gt;

&lt;p&gt;Theta ($\Theta$) é um conjunto de funções. Nesse caso, o conjunto das funções lineares. Por isso, é um abuso dizer que $2*n + 1$ &lt;strong&gt;é&lt;/strong&gt; $\Theta(n)$. A maneira formal de dizer é: $2*n + 1$ &lt;strong&gt;pertence&lt;/strong&gt; à $\Theta(n)$.&lt;/p&gt;

&lt;p&gt;Além disso, poderíamos ter escolhido qualquer função linear para dizer que $2 * n + 1$ tem a mesma ordem de crescimento. Nós escolhemos $n$ porque é a mais simples.&lt;/p&gt;

&lt;hr /&gt;

&lt;h1 id=&#34;a-notação-theta&#34;&gt;A notação $\Theta$&lt;/h1&gt;

&lt;p&gt;Agora vamos definir formalmente o que significa essa notação. Para duas funções $f(n)$ e $g(n)$, dizemos que $f(n)$ é $\Theta(g(n))$ se&lt;/p&gt;

&lt;p align=&#34;center&#34;&gt;$0&lt;=c1*g(n)&lt;= f(n)&lt;= c2*g(n), \forall n&gt;=n0$&lt;/p&gt;

&lt;p&gt;Vamos entender o que essa inequação complicada quer nos dizer. Em um resumo bem simplista ela está dizendo que se a gente &amp;ldquo;imprensar&amp;rdquo; $f(n)$ com $g(n)$ multiplicada por duas constantes diferentes, dizemos que $f(n)$ é $\Theta(g(n))$.&lt;/p&gt;

&lt;p&gt;Vamos ao exemplo. Lembra da função que descreve o tempo de execução da busca linear? Vamos tentar demonstrar que essa função é $\Theta(n)$.&lt;/p&gt;

&lt;p align=&#34;center&#34;&gt;$f(n) = 3*c*n+3*c$&lt;/p&gt;

&lt;p&gt;O primeiro passo que vamos fazer é trocar as constantes por 1. Isso já foi dito antes. Usar $c$ ou 1 tem o mesmo efeito. Assim, temos:&lt;/p&gt;

&lt;p align=&#34;center&#34;&gt;$f(n) = 3n+3$&lt;/p&gt; 

&lt;p&gt;Agora vamos voltar a inequação. Como &amp;ldquo;desconfiamos&amp;rdquo; que $f(n) = 3n+3$ é $\Theta(n)$, escolhemos $g(n)=n$. Poderíamos escolher qualquer função linear para representar $g(n)$, escolhemos a função linear mais simples para facilitar nossa vida. Assim, a inequação fica:&lt;/p&gt;

&lt;p align=&#34;center&#34;&gt;$0&lt;=c1*n&lt;= 3n+3&lt;= c2*n, \forall n&gt;=n0$&lt;/p&gt;  

&lt;p&gt;Agora precisamos encontrar valores para $c1$ e $c2$ para que essa inequação seja verdadeira. Vamos tentar com c1=1 e c2=6.&lt;/p&gt;

&lt;p align=&#34;center&#34;&gt;$0&lt;=n&lt;= 3n+3&lt;= 6*n, \forall n&gt;=n0$&lt;/p&gt;  

&lt;p&gt;Se verificarmos com $n=1$, vemos que a inequação é verdadeira:&lt;/p&gt;

&lt;p align=&#34;center&#34;&gt;$0&lt;=1&lt;= 6&lt;= 6$&lt;/p&gt;  

&lt;p&gt;Não é difícil também notar que $\forall n &amp;gt; 1$ ela sempre será verdadeira. Conseguimos, então, demonstrar que $f(n) \in \Theta(n)$, pois $g(n)=n$ limita inferior e superiomente $f(n)$.&lt;/p&gt;

&lt;p&gt;Na verdade, todas as funções lineares são limitadas inferior e superiormente por $n$. No nosso linguajar, podemos dizer que todas as funções abaixo pertencem à $\Theta(n)$.&lt;/p&gt;

&lt;p align=&#34;center&#34;&gt;$7*n, 827643*n, 5n+21, 54n +1...$&lt;/p&gt;  

&lt;p&gt;Formalmente dizemos que $g(n)=n$ é um limite assintótico restrito para $f(n)$. A figura abaixo descreve essa relação entre uma função quadrática e as funções $3n$ e $n^2$.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;theta.png&#34; alt=&#34;theta&#34; width=&#34;1px&#34; height=&#34;320px&#34;/&gt;&lt;/p&gt;

&lt;p&gt;Em português estamos dizendo que existe, para grandes valores de $n$ e a partir de um número inteiro positivo $n0$, $c1$ e $c2$ tais que $c1*g(n)&amp;lt;= f(n)&amp;lt;= c2*g(n)$.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Em termos simplistas, $f(n) \in \Theta(g(n))$ significa dizer que o crescimento de f(n) é igual ao de g(n).&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;strong&gt;Outro exemplo.&lt;/strong&gt; Suponha que a função $7 * n^4 + 5 * n^2 +10$ descreva o custo de execução de um algoritmo. Se aplicarmos as abstrações simplificadoras, desconfiamos que $f(n) \in \Theta(n4)$, certo? Vamos demonstrar formalmente.&lt;/p&gt;

&lt;p align=&#34;center&#34;&gt;$0 &lt;= c1 * n^4 &lt;= 7 * n^4 + 5 * n^2 + 10 &lt;= c2*n4, \forall n &gt;= n0$&lt;/p&gt;  

&lt;p&gt;Se escolhermos $c1=7$, $c2=22$ e $n0=1$, temos:&lt;/p&gt;

&lt;p align=&#34;center&#34;&gt; $ 0 &lt;=7 &lt;= 22 &lt;= 22 $&lt;/p&gt;

&lt;p&gt;Na verdade, todas as funções quadráticas são limitadas inferior e superiormente por $n^2$. No nosso linguajar, podemos dizer que todas as funções abaixo pertencem à $\Theta(n^2)$.&lt;/p&gt;

&lt;p align=&#34;center&#34;&gt; $ 43 * n^2 + 7n + 1, 5 * n^2 + 21, 7 * n^2...$ &lt;/p&gt;

&lt;p&gt;Em resumo, para demonstrar formalmente precisamos dos seguintes passos:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;Aplicar as abstrações simplificadores em $f(n)$ para termos uma proposta para $g(n)$.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Encontrar valores de $c1$, $c2$ e $n0$ para os quais a inequação $0&amp;lt;=c1*g(n)&amp;lt;= f(n)&amp;lt;= c2*g(n)$ é verdadeira.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Há mais 4(!) notações para estabelecer a relação entre funções. Neste material vamos ver apenas mais duas porque considero que é suficiente. Independente disso, todas são nada mais do que alterações na inequação que estabelecemos para $\Theta$. Por exemplo, a próxima notação que veremos, provavelmente a mais popular de todas, nada mais é do que retirar o limite inferior da inequação e apenas estabelecer um limite superior.&lt;/p&gt;

&lt;hr /&gt;

&lt;h1 id=&#34;notação-o-big-o-notation&#34;&gt;Notação O (Big O notation)&lt;/h1&gt;

&lt;p&gt;Enquanto a notação $\Theta$ define os limites inferior e superior de uma função, a notação $O$ define apenas o limite superior. Ou seja, define um teto para uma determinada função.&lt;/p&gt;

&lt;p&gt;Para duas funções $f(n)$ e $g(n)$, dizemos que $f(n)$ é $O(g(n))$ se:&lt;/p&gt;

&lt;p align=&#34;center&#34;&gt; $0&lt;=f(n)&lt;= c*g(n), \forall n&gt;=n0$ &lt;/p&gt;

&lt;p&gt;Veja que a diferença entre essa inequação e a utilizada para a notação é o fato de que aqui o limite inferior é 0 e não $c1 * g(n)$. A figura abaixo ilustra essa relação.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;bigo.png&#34; alt=&#34;bigo&#34; /&gt;&lt;/p&gt;

&lt;p&gt;O processo para demonstrar que $f(n)$ é $O(g(n))$ é muito semelhante, mas nesse caso precisamos achar apenas os valores de $c$ e $n0$.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Exemplo.&lt;/strong&gt; Suponha que a função $n^2 + 1$ descreva o custo de execução de um algoritmo. Se aplicarmos as abstrações simplificadoras, desconfiamos que $f(n) \in O(n^2)$, certo? Vamos demonstrar formalmente.&lt;/p&gt;

&lt;p align=&#34;center&#34;&gt; $0 &lt;= n^2 + 1 &lt;= c * n^2, \forall n&gt;=n0$ &lt;/p&gt;

&lt;p&gt;Se escolhermos c1=1 e n0=1, temos:&lt;/p&gt;

&lt;p align=&#34;center&#34;&gt; $1 &lt;= 1$ &lt;/p&gt;

&lt;p&gt;Como você pode perceber, toda função que pertence à $\Theta(n^2)$ também pertence à $O(n^2)$, porque $\Theta$ limita também superiormente como $O$. No entanto, nem toda função que pertence à $O(n^2)$, por exemplo, também pertence à $\Theta(n)$, pois $O$ estabelece apenas o limite superior. Por exemplo, a função $f(n) = 7$ é limitada superiormente por $n^2$ e, portanto, é $O(n^2)$. Todavia, não podemos dizer que ela é $\Theta(n^2)$ porque não há constante multiplicadora que, para n suficientemente grande, faça com que $c1 * n^2$ seja menor do que $7n$.&lt;/p&gt;

&lt;p&gt;Simples, não é? A notação $O$ é bastante utilizada em Computação para discutir a eficiência de algoritmos. E há aqui uma curiosidade. Como discutido no parágrafo anterior, basta escolhermos uma função com $n$ elevado a um expoente maior do que o da função sob análise que conseguimos definir um limite superior para ele. Por exemplo, a função $f(n) = n^2$ é $O(n^2)$, $O(n^3)$, $O(n^4)$, e assim por diante.  Todavia, faz mais sentido escolhermos uma função com o mesmo expoente, porque a informação é mais precisa. Ou seja, se uma função é quadrática, dizemos que ela é $O(n^2)$.&lt;/p&gt;

&lt;p&gt;Por fim, outra particularidade dessa notação é que usamos com muita frequência nas discussões do a dia a notação $O$ ao invés da notação $\Theta$. Talvez porque seja mais fácil de falar $O$ do que theta e, como somos preguiçosos, tendemos a economizar energia até na fala. Mas é relevante destacar que, tipicamente, a semântica que queremos empregar nas discussões com o uso da notação $O$ é a mesma de $\Theta$.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Em termos simplistas, $f(n) \in O(g(n))$ significa dizer que o crescimento de f(n) é menor ou igual ao crescimento de g(n).&lt;/p&gt;
&lt;/blockquote&gt;

&lt;hr /&gt;

&lt;h1 id=&#34;notação-omega-omega&#34;&gt;Notação Omega ($\Omega$)&lt;/h1&gt;

&lt;p&gt;A notação $\Theta$ define o limite inferior e superior. $O$ define apenas o limite superior. E $\Omega$? Acertou. Apenas o limite inferior. Para duas funções $f(n)$ e $g(n)$, dizemos que $f(n)$ é $\Omega(g(n))$ se:&lt;/p&gt;

&lt;p align=&#34;center&#34;&gt; $ 0 &lt;= c * g(n) &lt;= f(n), \forall n&gt;=n0$ &lt;/p&gt;

&lt;p&gt;A figura abaixo ilustra essa relação.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;omega.png&#34; alt=&#34;omega&#34; /&gt;&lt;/p&gt;

&lt;p&gt;O processo para demonstrar que $f(n)$ é $\Omega(g(n))$ é muito semelhante, mas nesse caso precisamos achar apenas os valores de $c$ e $n0$.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Exemplo.&lt;/strong&gt; Suponha que a função $n^2 + 1$ descreva o custo de execução de um algoritmo. Se aplicarmos as abstrações simplificadoras, desconfiamos que $f(n) \in \Omega(n^2)$, certo? Vamos demonstrar formalmente.&lt;/p&gt;

&lt;p align=&#34;center&#34;&gt; $ 0 &lt;= c * n^2 &lt;= n^2 + 1, \forall n &gt;= n0$ &lt;/p&gt;

&lt;p&gt;Se escolhermos $c1=1$ e $n0=1$, temos:&lt;/p&gt;

&lt;p align=&#34;center&#34;&gt; $ 1 &lt;= 2$ &lt;/p&gt;

&lt;p&gt;Não é difícil perceber que essa inequação é verdadeira para todo $n0$ maior do que 1.&lt;/p&gt;

&lt;p&gt;Como você pode perceber, toda função que pertence à $\Theta(n^2)$ também pertence à
$\Omega(n^2)$, porque $\Theta$ limita também inferiormente como $\Omega$. Contudo, nem toda função que pertence à $\Omega(n^2)$ também pertence à $\Theta(n^2)$, pois $\Omega(n^2)$ estabelece apenas o limite inferior. Por exemplo, a função $f(n) = 7 * n$ é limitada inferiormente por $n$, portanto é $\Omega(n)$. Contudo, não podemos dizer que ela é $\Theta(n^2)$ porque não há constante multiplicadora que para $n$ suficientemente grande faça com que $c1 * n$ seja maior do que $7 * n^2$.&lt;/p&gt;

&lt;p&gt;É simples definir um limite inferior para qualquer função. Basta utilizar o expoente 0. Ou seja, todas as funções são $\Omega(1)$. Mais do que isso, podemos escolher expoentes menores. Por exemplo, a função $f(n) = n^2$ é $\Omega(n)$, $\Omega(log n)$ e $\Omega(1)$. Todavia, faz mais sentido escolhermos uma função com o mesmo expoente, porque a informação é mais precisa. Ou seja, se uma função é quadrática, dizemos que ela é $\Omega(n^2)$.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Em termos simplistas, $f(n) \in \Omega(g(n))$ significa dizer que o crescimento de
f(n) é maior ou igual ao crescimento de g(n).&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;As duas notações restantes são $o$ (o minúsculo) e $\omega$ (omega minúsculo). Como disse, eu considero essas duas notações menos importantes que as demais e não vou discuti-las de forma aprofundada.&lt;/p&gt;

&lt;p&gt;** Apenas deixo aqui registrado que $o$ é muito semelhante à $O$, removendo apenas o sinal de igualdade da inequação:&lt;/p&gt;

&lt;p align=&#34;center&#34;&gt; $0&lt;=f(n)&lt; c*g(n), \forall n&gt;=n0$ &lt;/p&gt;

&lt;p&gt;Ou seja, &lt;strong&gt;não&lt;/strong&gt; podemos dizer, por exemplo, que $f(n) = n^2 + 3$ é $o(n^2)$. $f(n) = n^2 + 3$ é $o(n^3)$, $o(n^4)$, $o(n^5)$ etc.&lt;/p&gt;

&lt;p&gt;Por outro lado, $\omega$ é muito semelhante à $\Omega$, removendo apenas o sinal de igualdade da inequação:&lt;/p&gt;

&lt;p align=&#34;center&#34;&gt; $0 &lt;= c*g(n) &lt; f(n), n&gt;=n0$ &lt;/p&gt;

&lt;p&gt;Ou seja, &lt;strong&gt;não&lt;/strong&gt; podemos dizer, por exemplo, que $f(n) = n^3 + 2$ é $\omega(n^3)$. $f(n) = n^3 + 2$ é $\omega(n^2)$, $\omega(n)$, $\omega(log n)$ etc.&lt;/p&gt;

&lt;hr /&gt;

&lt;h1 id=&#34;notas&#34;&gt;Notas&lt;/h1&gt;

&lt;p&gt;Este material foi inspirado nos Capítulos 3 e 4 do livro &amp;ldquo;Algoritmos: Teoria e Prática&amp;rdquo; de Cormen et. al.&lt;/p&gt;

&lt;p&gt;Para entender este material é muito importante ler a &lt;a href=&#34;http://joaoarthurbm.github.io/eda/posts/introducao-a-analise&#34;&gt;introdução à análise de algoritmos&lt;/a&gt;.&lt;/p&gt;
</description>
       </item>
       
       <item>
         <title>Análise de Algoritmos Recursivos</title>
         <link>https://joaoarthurbm.github.io/eda/posts/analise-algoritmos-recursivos/</link>
         <pubDate>Mon, 28 Oct 2019 00:00:00 -0300</pubDate>
         
         <guid>https://joaoarthurbm.github.io/eda/posts/analise-algoritmos-recursivos/</guid>
         <description>

&lt;hr /&gt;

&lt;p&gt;Até aqui vimos como &lt;a href=&#34;http://joaoarthurbm.github.io/eda/posts/introducao-a-analise&#34;&gt;analisar algoritmos iterativos&lt;/a&gt;, lembra? Esse processo pode ser resumido pelos seguintes passos:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;identificar operações primitivas;&lt;/li&gt;
&lt;li&gt;identificar a quantidade de vezes que cada uma dessas primitivas é executada;&lt;/li&gt;
&lt;li&gt;Somar essas execuções.&lt;/li&gt;
&lt;/ol&gt;

&lt;pre&gt;
Você lembra quais são as operações primitivas?
    
    - Avaliação de expressões booleanas;
    - Operações matemáticas;
    - Retorno de métodos;
    - Atribuição;
    - Acesso à variáveis e posições arbitrárias de um array
&lt;/pre&gt;

&lt;p&gt;Seguindo esses passos sempre chegamos a uma função que descreve o tempo de execução do algoritmo. Vimos também que estamos interessados na ordem de crescimento dessa função, mais do que nos seus termos detalhados. Isto é, como se comporta a função para grandes valores de $n$. Assim, podemos aplicar as seguintes diretrizes para identificar a classe de complexidade das funções:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Eliminar constantes;&lt;/li&gt;
&lt;li&gt;Eliminar expoentes de menor magnitude.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Desse modo, a função $f(n) = 70n + 32n + 231$ tem ordem de crescimento linear. Isto é, $f(n) \in \Theta(n)$. Lembrando sempre que a maneira formal de demonstrar que $f(n) \in \Theta(n)$ é encontrar $c1$, $c2$ e $n0$, tal que:&lt;/p&gt;

&lt;p align=&#34;center&#34;&gt; $0 &lt;= c1*n  &lt;= 70n + 32n + 231 &lt;= c2*n, \forall n &gt;=n0 $ &lt;/p&gt;

&lt;hr /&gt;

&lt;h1 id=&#34;o-problema&#34;&gt;O Problema&lt;/h1&gt;

&lt;p&gt;Acontece que, para algoritmos recursivos, a aplicação dos passos acima não é direta, pois um algoritmo recursivo é definido em termos dele mesmo. Vamos começar com uma função bem simples: fatorial.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-java&#34; data-lang=&#34;java&#34;&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;public&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;int&lt;/span&gt; fatorial(&lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;n&lt;/span&gt;) {
    &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; (n&lt;span style=&#34;color:#f92672&#34;&gt;==&lt;/span&gt;0 &lt;span style=&#34;color:#f92672&#34;&gt;||&lt;/span&gt; n &lt;span style=&#34;color:#f92672&#34;&gt;==&lt;/span&gt; 1)
        &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; 1;
   &lt;span style=&#34;color:#66d9ef&#34;&gt;else&lt;/span&gt;
       &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; n &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt; fatorial(n&lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;1);
}&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Vamos tentar aplicar os passos que aprendemos para a análise de algoritmos.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Identificando as primitivas.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;if (n==0 || n == 1)&lt;/code&gt; -&amp;gt; avaliação de expressão booleana.&lt;/p&gt;

&lt;p&gt;&lt;code&gt;return 1&lt;/code&gt; -&amp;gt; retorno de método.&lt;/p&gt;

&lt;p&gt;&lt;code&gt;return n&lt;/code&gt; -&amp;gt; retorno de método.&lt;/p&gt;

&lt;p&gt;&lt;code&gt;*&lt;/code&gt; -&amp;gt; operação aritmética.&lt;/p&gt;

&lt;p&gt;&lt;code&gt;fatorial(n-1)&lt;/code&gt; -&amp;gt; ?&lt;/p&gt;

&lt;p&gt;Como vimos, para o caso em que as execuções não são em função de $n$ (caso acima) podemos simplificar as operações primitivas e suas execuções para (1).&lt;/p&gt;

&lt;p align=&#34;center&#34;&gt;O problema aqui é calcular o custo de fatorial(n-1).&lt;/p&gt;

&lt;p&gt;Qual o custo dessa operação e quantas vezes ela será executada? Não conseguimos responder essa questão de maneira direta como fizemos para os algoritmos iterativos porque trata-se de uma função definida em termos dela mesma. No nosso contexto, funções dessa natureza são chamadas de &lt;strong&gt;relações de recorrência&lt;/strong&gt;.&lt;/p&gt;

&lt;hr /&gt;

&lt;h1 id=&#34;relação-de-recorrência&#34;&gt;Relação de Recorrência&lt;/h1&gt;

&lt;blockquote&gt;
&lt;p&gt;Relação de recorrência é uma equação ou inequação que descreve uma função em termos dela mesma considerando entradas menores.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;A função que descreve o tempo de execução de um algoritmo recursivo é dada por sua relação de recorrência. Vejamos a relação de recorrência que descreve o algoritmo de cálculo do fatorial:&lt;/p&gt;

&lt;p&gt;$T(n) = T(n-1) + \Theta(1)$,&lt;/p&gt;

&lt;p&gt;simplificando temos: $T(n) = T(n-1) + 1$&lt;/p&gt;

&lt;p&gt;Ou seja, o custo de calcular fatorial(n) é o custo de calcular fatorial(n-1) somado às primitivas que são executadas a cada passo da recursão que, nesse caso, representam 1.&lt;/p&gt;

&lt;p&gt;Nosso desafio então é resolver essa relação de recorrência para determinarmos o tempo de execução do algoritmo para cálculo do fatorial. Para isso, vamos utilizar o método da árvore de recursão.&lt;/p&gt;

&lt;hr /&gt;

&lt;h1 id=&#34;método-da-árvore-de-recursão&#34;&gt;Método da árvore de recursão&lt;/h1&gt;

&lt;p&gt;A ideia para resolver uma relação de recorrência é simular a sua execução através de uma árvore, onde os nós representam a entrada e as arestas representam a chamada recursiva.&lt;/p&gt;

&lt;h2 id=&#34;exemplo-fatorial&#34;&gt;Exemplo: Fatorial&lt;/h2&gt;

&lt;p&gt;Vamos entender como funciona esse recurso através de exemplos. Veja a árvore de recursão para o cálculo do fatorial de 5.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;fatorial.png&#34; alt=&#34;fatorial&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Note que a raiz da árvore inicia com o valor 5, que é o tamanho da entrada. Note também que o custo do nível da entrada 5 é 1 (as primitivas). Este custo deve ser somado ao custo para a entrada 4 (chamada recursiva) que, por sua vez é 1. O cálculo da entrada 4 deve ser somado ao custo para a entrada 3 (chamada recursiva) e assim por diante. Veja que isso nada mais é do que a reprodução da relação de recorrência $T(n) = T(n-1) + 1$.&lt;/p&gt;

&lt;p&gt;Por fim, não é difícil compreender que o custo total é a soma dos custos de cada nível, ou seja, a soma dos custos de cada passo da recursão.&lt;/p&gt;

&lt;p&gt;Contudo, nosso trabalho aqui é definir o tempo de execução do algoritmo em função de uma entrada de tamanho n qualquer. Vamos, novamente, ilustrar a árvore de recursão para esse cenário:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;fatorialn.png&#34; alt=&#34;fatorialn&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Novamente, para calcular a função que define o tempo de execução desse algoritmo, precisamos somar os custos de cada nível. Isto é, somaremos o valor 1 uma quantidade de vezes representada por $h + 1$, onde $h$ é a altura da árvore e o +1 é o custo da última execução (&lt;code&gt;if n == 0 || n == 1&lt;/code&gt;).&lt;/p&gt;

&lt;p&gt;Portanto, precisamos definir $h$. Analisando a árvore, não é difícil notar que $h = n - 1$. Assim, temos que $f(n) = 1 * (n-1) + 1$, isto é, $f(n) = n$. Portanto, podemos dizer que $f(n) \in \Theta(n)$.&lt;/p&gt;

&lt;p&gt;Em resumo, podemos estabelecer os seguintes passos para analisar um algoritmo recursivo:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Estabelecer a relação de recorrência&lt;/li&gt;
&lt;li&gt;Expandir a árvore de execução baseado na relação de recorrência&lt;/li&gt;
&lt;li&gt;Determinar a altura h máxima da árvore&lt;/li&gt;
&lt;li&gt;Somar o custo de cada nível de execução&lt;/li&gt;
&lt;li&gt;Somar o custo total (soma do custo de todos os níveis)&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&#34;exemplo-mergesort&#34;&gt;Exemplo: MergeSort&lt;/h2&gt;

&lt;p&gt;Vamos analisar um exemplo um pouco mais complexo. O Merge Sort é um algoritmo de ordenação que, a cada execução parcial, efetua duas chamadas recursivas diminuindo pela metade o tamanho da entrada e um rotina (merge) cujo tempo de execução é dado por $\Theta(n)$.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-java&#34; data-lang=&#34;java&#34;&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;public&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;void&lt;/span&gt; mergeSort(&lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt;[] &lt;span style=&#34;color:#a6e22e&#34;&gt;v&lt;/span&gt;, &lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;ini&lt;/span&gt;, &lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;fim&lt;/span&gt;) {
	If (ini &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&lt;/span&gt; fim) {
		&lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;meio&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; (ini &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; fim) &lt;span style=&#34;color:#f92672&#34;&gt;/&lt;/span&gt; 2;
		mergeSort(v, ini, meio);
		mergeSort(v, meio &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; 1, fim);
		merge(v, ini, meio, fim);
	}
}&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;strong&gt;Relação de recorrência.&lt;/strong&gt; A primeira etapa para identificar a classe de complexidade do &lt;em&gt;Merge Sort&lt;/em&gt; é identificar a sua relação de recorrência:&lt;/p&gt;

&lt;p&gt;$T(n) = T(n/2) + T(n/2) + (n)$, simplificando&lt;/p&gt;

&lt;p&gt;$T(n) = 2 * T(n/2) + n$, onde n = v.length - 1&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;$2 * T(n/2)$ representa as duas chamadas recursivas em que a entrada é divida pela metade em cada uma delas.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;$+ n$ representa o custo da função que une duas sequências já ordenadas em uma sequência ordenada. Não precisamos saber como isso é feito nesse momento, apenas precisamos saber que essa parte do algoritmo tem custo linear.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;
Para fixar! Muitas relações de recorrência podem ser descritas 
na seguinte forma:

T(n) = a*T(n/b) + f(n)  , com a&gt;=1, b&gt;1 e f(n) não negativa. 

É importante que a gente saiba em português o que significa essa 
equação acima. Você lembra que ela é referente a um algoritmo 
recursivo, certo? Em português, dizemos que há a chamadas 
recursivas e que cada chamada recursiva divide a entrada em b 
partes. Além disso, a cada chamada recursiva, um custo f(n) é 
adicionado.
&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;Árvore de Recursão.&lt;/strong&gt; Vamos ilustrar a árvore de recursão gerada pela recorrência $T(n) = 2 T(n/2) + n$.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;merge.png&#34; alt=&#34;merge&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Podemos notar que a árvore é um pouco diferente da que ilustramos para o fatorial. Em primeiro lugar, a árvore é binária. Sendo assim, o custo de um nível agora é calculado somando-se os custos de cada nó desse nível. Novamente, as arestas representam as duas chamadas recursivas de cada passo. Outra mudança é que cada nó filho diminui na metade o tamanho da entrada do nó pai. Essas duas últimas sentenças são resumidas por $2 * T (n/2)$. Por fim, cada nó tem o seu tempo de execução definido em função linear do tamanho da entrada. Essa última sentença é resumida pela parte final da relação de recorrência &amp;hellip;. $+n$.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Função do tempo de execução.&lt;/strong&gt; Agora precisamos somar os custos de todos os níveis. Para isso, assim como no caso do fatorial, precisamos determinar a altura dessa árvore.&lt;/p&gt;

&lt;p&gt;Para o cálculo da altura podemos notar que a árvore irá parar de crescer quando $n / 2^h = 1$, pois o algoritmo atinge a condição de parada ini &amp;gt;= fim.&lt;/p&gt;

&lt;p&gt;Assim, temos que $2^h = n$. Aplicando $\log_{2}2$ nos dois lados da equação, temos:&lt;/p&gt;

&lt;p&gt;$h * \log_{2}2 = \log n$&lt;/p&gt;

&lt;p&gt;Simplificando, temos: $h = \log_{2}n$&lt;/p&gt;

&lt;p&gt;Agora que já definimos a altura da árvore, precisamos somar os custos parciais (de cada nível) uma quantidade de vezes representada pela altura da árvore. Cada nível tem custo $n$ (ex: $2 * n/2$, $4 * n/4$, $8 * n/8$&amp;hellip;). Se somarmos $n$ por 10 vezes, teremos $10*n$. Se somarmos $n$ por 100 vezes, teremos $100*n$. Como vamos somar $n$ por $\log n$ vezes, temos que o tempo de execução desse algoritmo é dado por $f(n) = n * \log n$. Naturalmente, só podemos fazer essa multiplicação porque cada nível tem o mesmo custo n.&lt;/p&gt;

&lt;p&gt;Então, temos que $T(n) =(n * \log n)$.&lt;/p&gt;

&lt;h2 id=&#34;exemplo-busca-binária&#34;&gt;Exemplo: Busca Binária&lt;/h2&gt;

&lt;p&gt;O algoritmo de busca binária é um algoritmo clássico de identificação da posição de um determinado elemento em uma sequência ordenada. A ideia é &amp;ldquo;palpitar&amp;rdquo; sempre a posição central. Caso o palpite seja maior do que o valor sendo procurado, o algoritmo descarta a metade à frente do palpite e passa a procurar na metade que contém os valores menores do que o palpite. Dessa maneira, a cada passo da recursão, são descartados metade dos elementos restantes. Esse procedimento torna a busca binária muito eficiente, quando comparada com a busca linear, que descarta apenas um elemento a cada iteração.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-java&#34; data-lang=&#34;java&#34;&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;public&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;int&lt;/span&gt; indexOf(&lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt;[] &lt;span style=&#34;color:#a6e22e&#34;&gt;v&lt;/span&gt;, &lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;n&lt;/span&gt;, &lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;ini&lt;/span&gt;, &lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;fim&lt;/span&gt;) {
    &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; (ini &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&lt;/span&gt; fim) {    
        &lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;meio&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; ini &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; fim;

        &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; (v[meio] &lt;span style=&#34;color:#f92672&#34;&gt;==&lt;/span&gt; n) &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; meio;

        &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; (n &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&lt;/span&gt; v[meio])
            &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; indexOf(v, n, ini, meio &lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt; 1);
        &lt;span style=&#34;color:#66d9ef&#34;&gt;else&lt;/span&gt;
            &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; indexOf(v, n, meio &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; 1, fim);
	} &lt;span style=&#34;color:#66d9ef&#34;&gt;else&lt;/span&gt; {
		&lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;1;
	}
}&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;strong&gt;Relação de recorrência.&lt;/strong&gt; Como aprendemos anteriormente, a primeira etapa para identificar o custo de execução do algoritimo de Busca Binária é identificar a sua relação de recorrência:&lt;/p&gt;

&lt;p&gt;$T(n) = T(n/2) + \Theta(1)$.&lt;/p&gt;

&lt;p&gt;Simplificando, $T(n) =  T(n/2) + 1$, onde n = v.length - 1&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;$T(n/2)$ representa a chamada recursiva em que a entrada é divida pela metade. Importante notar aqui que, embora haja duas chamadas recursivas no código, apenas uma é executada a cada passo. Por isso temos $T(n/2)$ e não $2 * T(n/2)$.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;$+ 1$ representa o custo da operação de cálculo do meio e da avaliação das expressões booleanas.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Árvore de Recursão. Vamos ilustrar a árvore de recursão gerada pela recorrência $T(n) = T(n/2) + 1$.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;binaria.png&#34; alt=&#34;binaria&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Cada nó da árvore possui apenas uma aresta, porque há apenas uma chamada recursiva. Cada nível tem o seu custo constante (1), uma vez que a cada passo da recursão apenas algumas primitivas são executadas, como as avaliações das expressões booleanas e o cálculo da variável &lt;code&gt;meio&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Função do tempo de execução.&lt;/strong&gt; Agora precisamos somar os custos de todos os níveis. Para isso, assim como nos casos anteriores, precisamos determinar a altura dessa árvore.&lt;/p&gt;

&lt;p&gt;O cálculo da altura é exatamente o mesmo do realizado para o exemplo do Merge Sort. A árvore irá parar de crescer quando $n / 2^h = 1$, pois o algoritmo atinge a condição de parada &lt;code&gt;ini &amp;gt;= fim&lt;/code&gt;. Aplicando os mesmos passos do exemplo anterior, temos que a $h = \log n$&lt;/p&gt;

&lt;p&gt;Agora que já definimos a altura da árvore, precisamos somar os custos parciais (de cada nível) uma quantidade de vezes representada pela altura da árvore. Cada nível tem custo 1. Se somarmos 1 por 10 vezes, teremos $10*1$. Se somarmos 1 por 100 vezes, teremos $100*1$. Como vamos somar 1 por $\log n$ vezes, temos que o tempo de execução desse algoritmo é dado por $f(n) = 1 * \log n$, ou seja, $f(n) = \log n$. Naturalmente, só podemos fazer essa multiplicação porque cada nível tem o mesmo custo 1.&lt;/p&gt;

&lt;hr /&gt;

&lt;h1 id=&#34;método-mestre&#34;&gt;Método Mestre&lt;/h1&gt;

&lt;p&gt;O método iterativo utilizando a árvore de recursão é, de fato, uma boa alternativa para identificar a classe de complexidade de algoritmos recursivos. Além de ser um método analítico, ele tem propriedades didáticas importantes. Isto é, o exercício de ilustrar a árvore de recursão (execução) e, a partir dela, identificar o custo total do algoritmo é importante não somente para esse fim, mas para exercitar a capacidade de abstração e raciocínio do aluno. Contudo, muitas vezes, trata-se de um mecanismo laborioso. Nesse contexto, surge o &lt;strong&gt;Teorema Mestre&lt;/strong&gt; que nos permite identificar a classe de complexidade de um algoritmo aplicando apenas algumas operações matemáticas e comparando ordem de complexidade de funções.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;E como o teorema funciona?&lt;/strong&gt; Primeiramente, é preciso que a relação de recorrência tenha determinadas propriedades. Vamos analisar concretamente essas propriedades:&lt;/p&gt;

&lt;p align=&#34;center&#34;&gt;$T(n) = a*T(n/b) + f(n)$&lt;/p&gt;

&lt;p&gt;Sendo $a&amp;gt;=1$, $b&amp;gt;1$ e $f(n)$ não negativa.&lt;/p&gt;

&lt;p&gt;Como vimos anteriormente, $a$ representa a quantidade de chamadas recursivas (quantidade de subproblemas), $b$ representa em quanto a entrada é diminuída a cada chamada recursiva e $f(n)$ representa o custo parcial de cada etapa da recursão. Para aplicar o Teorema Mestre, sua relação de recorrência deve ser na forma acima com $a &amp;gt;= 1$, $b &amp;gt; 1$ e $f(n)$ não negativa.&lt;/p&gt;

&lt;p&gt;Para esses casos, o Teorema Mestre é uma maneira direta de resolvermos a relação de recorrência. O Teorema Mestre estabelece que:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Se f(n) &amp;lt; n ** log&lt;sub&gt;b&lt;/sub&gt;&lt;sup&gt;a&lt;/sup&gt;, então T(n) = theta(n ** log&lt;sub&gt;b&lt;/sub&gt;&lt;sup&gt;a&lt;/sup&gt;).&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Se f(n) = n ** log&lt;sub&gt;b&lt;/sub&gt;&lt;sup&gt;a&lt;/sup&gt;, então T(n) = theta(f(n) * log&lt;sub&gt;b&lt;/sub&gt;&lt;sup&gt;n&lt;/sup&gt;).&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Se f(n) &amp;gt; n ** log&lt;sub&gt;b&lt;/sub&gt;&lt;sup&gt;a&lt;/sup&gt;, então T(n) = theta(f(n)).&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Desse modo, se a relação de recorrência obedecer às restrições $a&amp;gt;=1$, $b&amp;gt;1$ e $f(n)$ não negativa, basta aplicarmos o teorema.&lt;/p&gt;

&lt;h3 id=&#34;exemplo&#34;&gt;Exemplo&lt;/h3&gt;

&lt;p&gt;Para a relação de recorrência $T(n) = 8 * T(n/2) + 1000 * n^2$, temos:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;$a = 8$&lt;/li&gt;
&lt;li&gt;$b = 2$&lt;/li&gt;
&lt;li&gt;$f(n) = 1000 * n^2$&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Comparando $1000 * n^2$  com  n ** log&lt;sub&gt;b&lt;/sub&gt;&lt;sup&gt;a&lt;/sup&gt;, temos que $1000 * n^2$ &amp;lt; $n^3$. Portanto, aplicando a primeira regra do Teorema Mestre, podemos afirmar que T(n) = theta(n ** log&lt;sub&gt;b&lt;/sub&gt;&lt;sup&gt;a&lt;/sup&gt;) e, portanto, $T(n) = (n^3)$.&lt;/p&gt;

&lt;h3 id=&#34;exemplo-1&#34;&gt;Exemplo&lt;/h3&gt;

&lt;p&gt;$T(n) = 2 * T(n/2) + 10*n$&lt;/p&gt;

&lt;p&gt;Para a relação acima, temos:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;$a = 2$&lt;/li&gt;
&lt;li&gt;$b = 2$&lt;/li&gt;
&lt;li&gt;$f(n) = 10 * n$&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Comparando $10 * n$ com n ** log&lt;sub&gt;b&lt;/sub&gt;&lt;sup&gt;a&lt;/sup&gt; temos que $10 * n  =  n$, pois comparamos a ordem de grandeza das funções e, quando fazemos isso, as constantes não importam. Portanto, aplicando a segunda regra do Teorema Mestre, podemos afirmar que $T(n) = \Theta(n * \log_{2}n)$.&lt;/p&gt;

&lt;h3 id=&#34;exemplo-2&#34;&gt;Exemplo&lt;/h3&gt;

&lt;p&gt;Para $T(n) = 2 * T(n/2) + n^2$, temos:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;$a = 2$&lt;/li&gt;
&lt;li&gt;$b = 2$&lt;/li&gt;
&lt;li&gt;$f(n) = n^2$&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Comparando $n^2$  com n ** log&lt;sub&gt;b&lt;/sub&gt;&lt;sup&gt;a&lt;/sup&gt; temos que $n^2 &amp;gt; n$. Portanto, aplicando a terceira regra do Teorema Mestre, podemos afirmar que $T(n) = \Theta(f(n))$ e, portanto, $T(n) = \Theta(n^2)$.&lt;/p&gt;

&lt;hr /&gt;

&lt;h1 id=&#34;notas&#34;&gt;Notas&lt;/h1&gt;

&lt;p&gt;Este material é um resumo superficial do Capítulo 4 do livro &amp;ldquo;Algoritmos: Teoria e Prática&amp;rdquo; de Cormen et. al.&lt;/p&gt;

&lt;p&gt;Há outras implementações de fatorial. Por exemplo, ao invés de checar se n == 0 ou n == 1, bastaria apenas checar se n == 0, 1 * 1 == 1. Dessa forma, a altura da árvore gerada teria uma unidade a mais. Contudo, isso não impacta na ordem de grandeza do algoritmo.&lt;/p&gt;
</description>
       </item>
       
       <item>
         <title>Ordenação Linear</title>
         <link>https://joaoarthurbm.github.io/eda/posts/ordenacao-linear/</link>
         <pubDate>Sun, 27 Oct 2019 00:00:00 -0300</pubDate>
         
         <guid>https://joaoarthurbm.github.io/eda/posts/ordenacao-linear/</guid>
         <description>

&lt;hr /&gt;

&lt;p&gt;Os algoritmos de ordenação que vimos até então utilizam comparação para estabelecer a ordem entre os elementos de uma sequência. Primeiro vimos três algoritmos $\Theta(n^2)$: Selection Sort, Insertion Sort e Bubble Sort. Depois vimos alguns algoritmos $\Theta(n * \log n)$: Merge Sort e Quick Sort&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:Importante-lembr&#34;&gt;&lt;a href=&#34;#fn:Importante-lembr&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;

&lt;p&gt;Neste material vamos abordar algoritmos que não utilizam comparação, mas que são muito eficientes do ponto de vista de tempo de execução, embora demandem substancialmente mais memória do que o Selection Sort, Insertion Sort, Quick Sort etc.&lt;/p&gt;

&lt;h1 id=&#34;ordenação-por-contagem&#34;&gt;Ordenação por Contagem&lt;/h1&gt;

&lt;p&gt;Algo que chama a atenção em um primeiro momento é:&lt;/p&gt;

&lt;p align=&#34;center&#34;&gt;&lt;b&gt;Como é possível ordenar elementos sem utilizar comparação?&lt;/b&gt;&lt;/p&gt;

&lt;p&gt;Em geral, a ideia é valer-se do fato de que estamos ordenando números inteiros e que os índices dos arrays também são inteiros. Dessa maneira, podemos mapear o valor presente em uma sequência para a posição de mesmo valor em um array auxiliar (&lt;code&gt;array[i] = i&lt;/code&gt;). Essa é a estratégia geral dos algoritmos de ordenação linear que se baseiam na contagem dos elementos da sequência a ser ordenada.&lt;/p&gt;

&lt;p&gt;Antes de analisarmos os algoritmos de contagem em detalhes, vamos abordar um exemplo bem simples para entender esse conceito. Para isso, vamos entrar em um mundo em que:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;todos os elementos do array que vamos ordenar são inteiros positivos (1, 2, 3…k);&lt;/li&gt;
&lt;li&gt;não há repetição de elementos no array que vamos ordenar;&lt;/li&gt;
&lt;li&gt;sabemos o maior valor desse array, o qual chamamos de k.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Desse modo, se quisermos ordenar o array $A = [7, 2, 1, 4]$&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:O-ideal-nomear-v&#34;&gt;&lt;a href=&#34;#fn:O-ideal-nomear-v&#34;&gt;2&lt;/a&gt;&lt;/sup&gt;, basta criarmos um array auxiliar $C$ cujo tamanho é $k$, onde $k$ é o maior elemento do array original (7), e iterarmos sobre $A$ registrando a presença de seus elementos em $C$ através da seguinte instrução &lt;code&gt;C[A[i] - 1] = true&lt;/code&gt;. O índice é subtraído de 1, pois as posições de um array em Java iniciam-se de 0.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-java&#34; data-lang=&#34;java&#34;&gt;...
&lt;span style=&#34;color:#66d9ef&#34;&gt;boolean&lt;/span&gt;[] &lt;span style=&#34;color:#a6e22e&#34;&gt;C&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;new&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;boolean&lt;/span&gt;[k];

&lt;span style=&#34;color:#75715e&#34;&gt;// registrando a presença de A[i] na sequência
&lt;/span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; (&lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;i&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; 0; i &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&lt;/span&gt; A.&lt;span style=&#34;color:#a6e22e&#34;&gt;length&lt;/span&gt;; i&lt;span style=&#34;color:#f92672&#34;&gt;++&lt;/span&gt;) {
    C[A[i] &lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt; 1] &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;true&lt;/span&gt;;
}
...&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Se $A = [7, 2, 1, 4]$, com $k = 7$, temos $C$ preenchido da seguinte maneira:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;C = [&lt;span style=&#34;color:blue&#34;&gt;true&lt;/span&gt;, &lt;span style=&#34;color:blue&#34;&gt;true&lt;/span&gt;, false, &lt;span style=&#34;color:blue&#34;&gt;true&lt;/span&gt;, false, false, &lt;span style=&#34;color:blue&#34;&gt;true&lt;/span&gt;]&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Agora, se criarmos um array $B$ do tamanho de $A$ e iterarmos sobre o array $C$ preenchendo $B$ com o valor do índice $i + 1$ em que &lt;code&gt;C[i] == true&lt;/code&gt;, temos que $B$ é a versão ordenada de $A$.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-java&#34; data-lang=&#34;java&#34;&gt;...
&lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;j&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; 0;
&lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt;[] &lt;span style=&#34;color:#a6e22e&#34;&gt;B&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;new&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt;[A.&lt;span style=&#34;color:#a6e22e&#34;&gt;length&lt;/span&gt;];

&lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; (&lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;i&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; 0; i &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&lt;/span&gt; C.&lt;span style=&#34;color:#a6e22e&#34;&gt;length&lt;/span&gt;; i&lt;span style=&#34;color:#f92672&#34;&gt;++&lt;/span&gt;) {
    &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; (C[i] &lt;span style=&#34;color:#f92672&#34;&gt;==&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;true&lt;/span&gt;) {
        B[j] &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; i &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; 1;
        j &lt;span style=&#34;color:#f92672&#34;&gt;+=&lt;/span&gt; 1;
    }
}
...&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Assim, para $A = [7, 2, 1, 4]$, temos:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;C = [&lt;span style=&#34;color:blue&#34;&gt;true&lt;/span&gt;, &lt;span style=&#34;color:blue&#34;&gt;true&lt;/span&gt;, false, &lt;span style=&#34;color:blue&#34;&gt;true&lt;/span&gt;, false, false, &lt;span style=&#34;color:blue&#34;&gt;true&lt;/span&gt;]&lt;/li&gt;
&lt;li&gt;B = [1, 2, 4, 7], representando a sequência de valores de $A$, porém ordenada.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Viu como foi fácil? Note que $B$ foi preenchido com os valores de $i+1$ em que C[i] ==  true. Ou seja, $B[0] = 1$, pois C[0] ==  true. $B[1] = 2$, pois C[1] ==  true. $B[2] = 4$, pois C[3] ==  true. Por fim, $B[3] = 7$, pois C[6] ==  true.&lt;/p&gt;

&lt;p&gt;Vamos unir os trechos de código mostrados acima em um método que recebe $A$ e $k$ e retorna um array $B$ que representa a ordenação dos elementos de $A$.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-java&#34; data-lang=&#34;java&#34;&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;public&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;static&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt;[] &lt;span style=&#34;color:#a6e22e&#34;&gt;sort&lt;/span&gt;(&lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt;[] &lt;span style=&#34;color:#a6e22e&#34;&gt;A&lt;/span&gt;, &lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;k&lt;/span&gt;) {
    
    &lt;span style=&#34;color:#66d9ef&#34;&gt;boolean&lt;/span&gt;[] &lt;span style=&#34;color:#a6e22e&#34;&gt;C&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;new&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;boolean&lt;/span&gt;[k];

    &lt;span style=&#34;color:#75715e&#34;&gt;// registrando a presença de A[i] na sequência
&lt;/span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; (&lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;i&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; 0; i &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&lt;/span&gt; A.&lt;span style=&#34;color:#a6e22e&#34;&gt;length&lt;/span&gt;; i&lt;span style=&#34;color:#f92672&#34;&gt;++&lt;/span&gt;) {
        C[A[i] &lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt; 1] &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;true&lt;/span&gt;;
    }
   
    &lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;j&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; 0;
    &lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt;[] &lt;span style=&#34;color:#a6e22e&#34;&gt;B&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;new&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt;[A.&lt;span style=&#34;color:#a6e22e&#34;&gt;length&lt;/span&gt;];

    &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; (&lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;i&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; 0; i &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&lt;/span&gt; C.&lt;span style=&#34;color:#a6e22e&#34;&gt;length&lt;/span&gt;; i&lt;span style=&#34;color:#f92672&#34;&gt;++&lt;/span&gt;) {
        &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; (C[i] &lt;span style=&#34;color:#f92672&#34;&gt;==&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;true&lt;/span&gt;) {
            B[j] &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; i &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; 1;
            j &lt;span style=&#34;color:#f92672&#34;&gt;+=&lt;/span&gt; 1;
        }
    }

    &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; B;   
}&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;h3 id=&#34;mais-um-exemplo-a-9-1-3-4-6-7&#34;&gt;Mais um exemplo: A = [9, 1, 3, 4, 6, 7]&lt;/h3&gt;

&lt;p&gt;Sempre lembrando que sabemos o valor de $k$ e que não há repetição dos elementos a serem ordenados.&lt;/p&gt;

&lt;p&gt;Para $A = [9, 1, 3, 4, 6, 7]$, temos:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;C = [&lt;span style=&#34;color:blue&#34;&gt;true&lt;/span&gt;, false, &lt;span style=&#34;color:blue&#34;&gt;true&lt;/span&gt;, &lt;span style=&#34;color:blue&#34;&gt;true&lt;/span&gt;, false, &lt;span style=&#34;color:blue&#34;&gt;true&lt;/span&gt;,  &lt;span style=&#34;color:blue&#34;&gt;true&lt;/span&gt;, false,  &lt;span style=&#34;color:blue&#34;&gt;true&lt;/span&gt;]&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;B = [1, 3, 4, 6, 7, 9], representando a sequência de valores de $A$, porém ordenada.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;counting-sort-e-se-houver-repetição-no-array&#34;&gt;Counting Sort: E se houver repetição no array?&lt;/h2&gt;

&lt;p&gt;Repetição de valores em um array a ser ordenado não é um cenário incomum, certo? O fato de não haver repetição nos permitiu criar um array C de booleanos e registrar a presença ou não de um elemento.&lt;/p&gt;

&lt;p align=&#34;center&#34;&gt;O que faríamos se houvesse repetição?&lt;/p&gt;

&lt;p&gt;Daí surge ordenação por contagem (&lt;em&gt;Counting Sort&lt;/em&gt;). A ideia geral é registrar a frequência dos elementos ao invés da simples presença. Isso faz com que o array $C$ passe a ser um array de inteiros, não de booleanos. O algoritmo do &lt;em&gt;Counting Sort&lt;/em&gt; é baseado na ideia que vimos, mas possui algumas modificações substanciais para permitir elementos repetidos e para manter a estabilidade. Em linhas gerais, o &lt;em&gt;Counting Sort&lt;/em&gt; possui os seguintes passos:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;registrar a frequência dos elementos de $A$ no array $C$;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Calcular a soma cumulativa de $C$. Esse passo registra, para cada elemento $x$ da entrada, o número de elementos menores ou iguais a $x$;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;iterar sobre $A$ do fim ao início registrando em $B$ o valor de $A$ com a seguinte instrução &lt;code&gt;B[C[A[i] - 1] -1] = A[i]&lt;/code&gt;. Não se assuste. Essa sequência de decrementos em 1 é devido ao fato de começarmos os índices de um array a partir de zero em Java.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Antes de entrarmos nos detalhes de código do algoritmo, vamos simular a execução de um exemplo.&lt;/p&gt;

&lt;h3 id=&#34;exemplo-a-1-9-1-3-4-7-6-7&#34;&gt;Exemplo: A = [1, 9, 1, 3, 4, 7, 6, 7]&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;Passo 1: Contagem de frequência em C.&lt;/strong&gt;&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-java&#34; data-lang=&#34;java&#34;&gt;...
        &lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt;[] &lt;span style=&#34;color:#a6e22e&#34;&gt;C&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;new&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt;[k];

        &lt;span style=&#34;color:#75715e&#34;&gt;// frequência
&lt;/span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;        &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; (&lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;i&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; 0; i &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&lt;/span&gt; A.&lt;span style=&#34;color:#a6e22e&#34;&gt;length&lt;/span&gt;; i&lt;span style=&#34;color:#f92672&#34;&gt;++&lt;/span&gt;) {
            C[A[i] &lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt; 1] &lt;span style=&#34;color:#f92672&#34;&gt;+=&lt;/span&gt; 1;
        }
...&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Para  $A = [1, 9, 1, 3, 4, 7, 6, 7]$ e $k = 9$, temos:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;C = [2, 0, 1, 1, 0, 1, 2, 0, 1], isto é, no array a ser ordenado há dois elementos de valor 1, nenhum elemento de valor 2, um elemento de valor 3 e assim por diante.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Passo 2: Soma cumulativa em C.&lt;/strong&gt;&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-java&#34; data-lang=&#34;java&#34;&gt;...
        &lt;span style=&#34;color:#75715e&#34;&gt;// cumulativa
&lt;/span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;        &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; (&lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;i&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; 1; i &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&lt;/span&gt; C.&lt;span style=&#34;color:#a6e22e&#34;&gt;length&lt;/span&gt;; i&lt;span style=&#34;color:#f92672&#34;&gt;++&lt;/span&gt;) {
            C[i] &lt;span style=&#34;color:#f92672&#34;&gt;+=&lt;/span&gt; C[i&lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;1];
        }
...&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Para $C = [2, 0, 1, 1, 0, 1, 2, 0, 1]$, após a execução do cálculo da cumulativa, temos $C = [2, 2, 3, 4, 4, 5, 7, 7, 8]$, isto é, no array a ser ordenado há:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;2 elementos menores ou igual a 1&lt;/li&gt;
&lt;li&gt;2 elementos menores ou iguais a 2&lt;/li&gt;
&lt;li&gt;3 elementos menores ou iguais a 3&lt;/li&gt;
&lt;li&gt;4 elementos menores ou iguais a 4&lt;/li&gt;
&lt;li&gt;4 elementos menores ou iguais a 5&lt;/li&gt;
&lt;li&gt;5 elementos menores ou iguais a 6&lt;/li&gt;
&lt;li&gt;7 elementos menores ou iguais a 7&lt;/li&gt;
&lt;li&gt;7 elementos menores ou iguais a 8&lt;/li&gt;
&lt;li&gt;8 elementos menores ou iguais a 9.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Passo 3: Iterar do fim ao início de $A$ registrando em $B$ os elementos.&lt;/strong&gt;&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-java&#34; data-lang=&#34;java&#34;&gt;...
        &lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt;[] &lt;span style=&#34;color:#a6e22e&#34;&gt;B&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;new&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt;[A.&lt;span style=&#34;color:#a6e22e&#34;&gt;length&lt;/span&gt;];

        &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; (&lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;i&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; A.&lt;span style=&#34;color:#a6e22e&#34;&gt;length&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt; 1; i &lt;span style=&#34;color:#f92672&#34;&gt;&amp;gt;=&lt;/span&gt; 0; i&lt;span style=&#34;color:#f92672&#34;&gt;--&lt;/span&gt;) {
            B[C[A[i] &lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt; 1] &lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;1] &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; A[i];
            C[A[i] &lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt; 1] &lt;span style=&#34;color:#f92672&#34;&gt;-=&lt;/span&gt; 1;
        }
...&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Essa parte pode ser confusa e acredito que para entendê-la precisamos de recursos visuais melhores do que o texto. Por isso, fiz o vídeo abaixo.&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=3bm7NgKJpj4&amp;feature=youtu.be&#34;&gt;
    &lt;img src=&#34;video.jpg&#34;&gt;
&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;implementação-do-counting-sort&#34;&gt;Implementação do Counting Sort&lt;/h2&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-java&#34; data-lang=&#34;java&#34;&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;public&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;static&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt;[] &lt;span style=&#34;color:#a6e22e&#34;&gt;countingSort&lt;/span&gt;(&lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt;[] &lt;span style=&#34;color:#a6e22e&#34;&gt;A&lt;/span&gt;, &lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;k&lt;/span&gt;) {
    
        &lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt;[] &lt;span style=&#34;color:#a6e22e&#34;&gt;C&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;new&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt;[k];

        &lt;span style=&#34;color:#75715e&#34;&gt;// frequência
&lt;/span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;        &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; (&lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;i&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; 0; i &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&lt;/span&gt; A.&lt;span style=&#34;color:#a6e22e&#34;&gt;length&lt;/span&gt;; i&lt;span style=&#34;color:#f92672&#34;&gt;++&lt;/span&gt;) {
            C[A[i] &lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt; 1] &lt;span style=&#34;color:#f92672&#34;&gt;+=&lt;/span&gt; 1;
        }
        
        &lt;span style=&#34;color:#75715e&#34;&gt;// cumulativa
&lt;/span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;        &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; (&lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;i&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; 1; i &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&lt;/span&gt; C.&lt;span style=&#34;color:#a6e22e&#34;&gt;length&lt;/span&gt;; i&lt;span style=&#34;color:#f92672&#34;&gt;++&lt;/span&gt;) {
            C[i] &lt;span style=&#34;color:#f92672&#34;&gt;+=&lt;/span&gt; C[i&lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;1];
        }

        &lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt;[] &lt;span style=&#34;color:#a6e22e&#34;&gt;B&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;new&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt;[A.&lt;span style=&#34;color:#a6e22e&#34;&gt;length&lt;/span&gt;];

        &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; (&lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;i&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; A.&lt;span style=&#34;color:#a6e22e&#34;&gt;length&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt; 1; i &lt;span style=&#34;color:#f92672&#34;&gt;&amp;gt;=&lt;/span&gt; 0; i&lt;span style=&#34;color:#f92672&#34;&gt;--&lt;/span&gt;) {
            B[C[A[i] &lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt; 1] &lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;1] &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; A[i];
            C[A[i] &lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt; 1] &lt;span style=&#34;color:#f92672&#34;&gt;-=&lt;/span&gt; 1;
        }

        &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; B;
    
}&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;h2 id=&#34;análise-do-counting-sort&#34;&gt;Análise do Counting Sort&lt;/h2&gt;

&lt;p&gt;O Counting Sort tem em sua implementação 3 laços principais. O primeiro percorre o array $A$ (tamanho $n$), o segundo percorre o array $C$ (tamanho $k$) e o terceiro percorre novamente o array $A$. Assim, temos:&lt;/p&gt;

&lt;p&gt;$T(n) = 2n + k$. Aplicando as diretrizes para análise assintótica, temos:
$T(n) = (n + k)$.&lt;/p&gt;

&lt;p&gt;O importante aqui é entender que o algoritmo tem seu tempo de execução linear em função do tamanho de $n$ e $k$, não somente do tamanho de $n$. Esse tempo de execução é substancialmente mais eficiente do que os outros algoritmos que vimos. Contudo, esse algoritmo também tem um custo associado ao uso de memória, pois precisa criar um array de contagem $C$ de tamanho igual a $k$ e o array $B$ a ser retornado de tamanho igual ao do array original. Ou seja, do ponto de vista de memória, também que o consumo é dado por $T(n) = (n + k)$.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;O que acontece se k for muito maior que n?&lt;/strong&gt; Vejamos um exemplo:&lt;/p&gt;

&lt;p&gt;$A = [1, 3, 2, 1, 9874392]$&lt;/p&gt;

&lt;p&gt;Veja que teríamos que criar o array de contagem $C$ de tamanho 9874392 mesmo tendo que ordenar apenas 5 elementos, o que seria muito ruim.&lt;/p&gt;

&lt;p&gt;Por outro lado, o que acontece se $k$ for muito menor que $n$? Vejamos um exemplo:&lt;/p&gt;

&lt;p&gt;A = [1, 3, 2, 1, 1, 5, 3, 2, 5, 4, 2, 1, 2, 1, 1, 2, 1, 4, 5, 2, 2, 3, 2]&lt;/p&gt;

&lt;p&gt;Veja que teríamos que criar o array de contagem C de tamanho 5 para ordenar um array com 23 elementos.  Isso pode ser ainda mais vantajoso se imaginarmos um cenário em que teremos, por exemplo, que ordenar todas as pessoas do mundo de acordo com sua idade. Nesse caso, temos um conjunto muito grande de dados (~7.7 bilhões), mas com um $k$ bem menor, pois a pessoa mais velha do mundo não ultrapassaria, nos dias de hoje, 125 anos. Isto é, $k$ é muito menor do que $n$.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;E se eu quiser usar o Counting Sort para ordenar sequências contendo valores iguais a zero e valores negativos?&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;O Counting Sort baseia-se na ideia de que um valor inteiro pode ser mapeado para o índice de mesmo valor em um array auxiliar. Essa estratégia nos impede, em um primeiro momento, de ordenar uma sequência com números negativos, pois o menor índice em um array é 0. Além disso, na nossa implementação inicial excluímos também elementos iguais a zero. Contudo, é possível fazer algumas mudanças simples no Counting Sort para que o mesmo passe a também ordenar sequências com esses valores.&lt;/p&gt;

&lt;p&gt;A ideia é simples: basta identificarmos o menor elemento do array (menor) e usar esse valor como um &amp;ldquo;salto&amp;rdquo; para adicionar os elementos. É uma ideia similar a fazer um shift para a direita em todos os elementos. O menor elemento array tem sua frequência registrada na posição zero. Vamos ver um exemplo:&lt;/p&gt;

&lt;p&gt;$A = [1,-3, 2, 1, 7]$, com k = 7 e menor = -3.&lt;/p&gt;

&lt;p&gt;Em primeiro lugar, o array de contagem $C$ já não varia de 0 a $k$, mas sim de 0 a $k - menor + 1$, porque temos que considerar que a frequência do elemento de valor -3 será registrada na posição 0, a do valor -2, na posição 1, a do valor -1 na posição 0 e assim por diante. Por isso, quando for preciso mapear os elementos de A em C e B, temos que usar o salto de |menor| (3, no nosso exemplo). O cálculo da frequência seria dado pelo seguinte código:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-java&#34; data-lang=&#34;java&#34;&gt;...
    &lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt;[] &lt;span style=&#34;color:#a6e22e&#34;&gt;C&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;new&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt;[maior &lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt; menor &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; 1];

    &lt;span style=&#34;color:#75715e&#34;&gt;// frequência
&lt;/span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; (&lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;i&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; 0; i &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&lt;/span&gt; A.&lt;span style=&#34;color:#a6e22e&#34;&gt;length&lt;/span&gt;; i&lt;span style=&#34;color:#f92672&#34;&gt;++&lt;/span&gt;) {
        C[A[i] &lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt; menor] &lt;span style=&#34;color:#f92672&#34;&gt;+=&lt;/span&gt; 1;
    }
...&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Para A = [1,-3, 2, 1, 7], temos C = [1, 0, 0, 0, 2, 1, 0, 0, 0, 0, 1]. Note que o primeiro índice é reservado para a frequência do menor elemento (-3) e não mais para 1. Além disso, como estamos também contando com a presença de elementos de valor 0 no array, trocamos a instrução &lt;code&gt;C[A[i] - 1] += 1&lt;/code&gt; por &lt;code&gt;C[A[i] - menor] += 1&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;A mesma mudança é considerada no restante da implementação, sempre aplicando &lt;code&gt;array[i]- menor&lt;/code&gt; para considerar o salto.&lt;/p&gt;

&lt;hr /&gt;

&lt;h1 id=&#34;notas&#34;&gt;Notas&lt;/h1&gt;

&lt;p&gt;Este material não é tão completo quanto o livro texto da disciplina. Sugiro também a leitura do Capítulo 8 do livro &amp;ldquo;Algoritmos: Teoria e Prática&amp;rdquo; de Cormen et. al.&lt;/p&gt;

&lt;p&gt;No curso de Estrutura de Dados da UFCG há ainda a discussão de outros algoritmos de ordenação linear, como o Radix Sort e o Bucket Sort.&lt;/p&gt;
&lt;div class=&#34;footnotes&#34;&gt;

&lt;hr /&gt;

&lt;ol&gt;
&lt;li id=&#34;fn:Importante-lembr&#34;&gt;Importante lembrar que o Quick Sort no pior caso tem seu tempo de execução descrito por uma função quadrática. &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:Importante-lembr&#34;&gt;&lt;sup&gt;[return]&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li id=&#34;fn:O-ideal-nomear-v&#34;&gt; O ideal é nomear variável com letra minúscula em Java. Contudo, para fins didáticos, utilizaremos letras maiúsculas. &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:O-ideal-nomear-v&#34;&gt;&lt;sup&gt;[return]&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
       </item>
       
     </channel>
   </rss>
